{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Vocabulary – When selecting a model, we distinguish 3 different parts of the data that we\n",
    "have as follows:\n",
    "\n",
    "### Training set  \n",
    "- Model is trained\n",
    "- Usually 80% of the dataset \n",
    "\n",
    "### Testing set\n",
    "- Model gives predictions\n",
    "- Unseen data\n",
    "\n",
    "### Validation set\n",
    "- Model is assessed\n",
    "- Usually 20% of the dataset\n",
    "- Also called hold-out\n",
    "or development set\n",
    "\n",
    "Once the model has been chosen, it is trained on the entire dataset and tested on the unseen\n",
    "test set.\n",
    "\n",
    "## Cross Validation\n",
    "Class of methods that estimate test error by holding out\n",
    "a subset of training data from the fitting process.\n",
    "\n",
    "- Validation Set:\n",
    "    \n",
    "split data into training set and validation set.\n",
    "Train model on training and estimate test error\n",
    "using validation. e.g. 80-20 split\n",
    "\n",
    "- Leave-One-Out CV (LOOCV): \n",
    "    \n",
    "split data intotraining set and validation set, \n",
    "but the validation set consists of 1 observation. \n",
    "Then repeat n-1 times until all observations have\n",
    "been used as validation. Test erro is the average \n",
    "of these n test error estimates.\n",
    "\n",
    "- k-Fold CV: \n",
    "    \n",
    "randomly divide data into k groups (folds) of\n",
    "approximately equal size. First fold is used as validation\n",
    "and the rest as training. Then repeat k times and find\n",
    "average of the k estimates.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "#svm just for visulace metrics for acuricy\n",
    "from sklearn import svm,metrics \n",
    "\n",
    "#this  is a diferent scaling values are ranges are \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#croos_val for accuracy\n",
    "from sklearn.model_selection import KFold,cross_val_score,LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=pd.read_csv(\"/home/cryzal/ml/dataset/iris.csv\")\n",
    "x=iris.drop(\"Species\", axis=1)\n",
    "y=iris[\"Species\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min max scaler means range is only 0  and 1\n",
    "\n",
    "x=MinMaxScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithmdefine talk later(dimension reduction)\n",
    "\n",
    "svc=svm.SVC(kernel='linear',C=1,gamma='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold validation \n",
    "\n",
    "## k-fold \n",
    "- Training on k − 1 folds and\n",
    "assessment on the remaining one\n",
    "- Generally k = 5 or 10\n",
    "\n",
    "\n",
    "Leave-p-out\n",
    "- Training on n − p observations and\n",
    "assessment on the p remaining ones\n",
    "- Case p = 1 is called leave-one-out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_fold set up for validation\n",
    "\n",
    "k_fold=KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "## 1. What is cross-validation? How to do it right?\n",
    "\n",
    "\n",
    "It’s a model validation technique for assessing how the results of a\n",
    "statistical analysis will generalize to an independent data set.\n",
    "Mainly used in settings where the goal is prediction and one wants\n",
    "to estimate how accurately a model will perform in practice. The\n",
    "goal of cross-validation is to define a data set to test the model\n",
    "in the training phase (i.e. validation data set) in order to limit\n",
    "problems like overfitting, and get an insight on how the model will\n",
    "generalize to an independent data set.\n",
    "Examples: leave-one-out cross validation, K-fold cross validation\n",
    "How to do it right?\n",
    "\n",
    "•the training and validation data sets have to be drawn from\n",
    "the same population\n",
    "\n",
    "•predicting stock prices: trained for a certain 5-year period,\n",
    "it’s unrealistic to treat the subsequent 5-year a draw from\n",
    "the same population\n",
    "\n",
    "•common mistake: for instance the step of choosing the kernel\n",
    "parameters of a SVM should be cross-validated as well\n",
    "\n",
    "\n",
    "## Cross-validation \n",
    "– Cross-validation, also noted CV, is a method that is used to select a\n",
    "model that does not rely too much on the initial training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.53333333, 0.93333333, 0.6       ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#built for cross_validation test\n",
    "result=cross_val_score(svc,x,y,cv=k_fold)\n",
    "result\n",
    "#array([1.  #accuracy for first fold  100%    , \n",
    "#1.  accuracy 100% second fold      , \n",
    "#0.53333333 accuracy 53% third fold,\n",
    "#0.93333333 accuracy 93%, 0.6  accuracy 60%     ])\n",
    "# np.mean(result)\n",
    "\n",
    "# we get 81%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAVEONEOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index,test_index in loo.split(x):\n",
    "    x_train,x_test = x[train_index],x[test_index]\n",
    "    y_train,y_test = y[train_index],y[test_index]\n",
    "    svc.fit(x_train,y_train)#fit the both training set in algorithm\n",
    "    y_predt = svc.predict(x_test)#predict using algorithm and  x_test testing set of x\n",
    "    acc=metrics.accuracy_score(y_test,y_predt)#to calculate accuracy using metrics and test set of y  and predict of algo y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
