{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm,datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel\n",
    "\n",
    "In machine learning, the kernel trick is a method, that is a class of pattern analysis and\n",
    "also provides better boundary classification by giving better visualizations.\n",
    "\n",
    "There are some data in lower-dimensional space, where the classification is linear.\n",
    "Kernel tricks take those data and project it into the higher dimensional space to get a nonlinear classification.\n",
    "Here it adds some more new columns, it does not classify anything. \n",
    "It’s just for getting good accuracy with better explanations.\n",
    "\n",
    "- In a 3dimensional space, the classifier is 2dimension. So, in a current dimension, \n",
    "we cannot use any classification or logistic regression. Here we can do one thing \n",
    "by projecting the data from lower dimensional space to higher-dimensional space,\n",
    "we can solve the existing issue i.e. the use of logistic regression. \n",
    "We can use logistic regression after applying kernel tricks.\n",
    "This is the advantage of the kernel tricks method.\n",
    "\n",
    "- Sometimes in kernel tricks, it is very difficult to find the perfect kernel.\n",
    "\n",
    "# C parameter\n",
    "\n",
    "### What is the Significance of C value in Support Vector Machine?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As we know, in Support Vector Machine we always look for 2 things:\n",
    "\n",
    "    Setting a larger margin\n",
    "    lowering misclassification rate(how much a model misqualifies a data)\n",
    "\n",
    "Now the problem is above said 2 things are kind of contradictory.\n",
    "If we increase margin, we will end up getting a high misclassfication rate on the other \n",
    "hand if we decrease a margin, we will end up getting a lower misclassification rate.\n",
    "\n",
    "You must be thinking then why we want a larger margin, our priority \n",
    "should be getting a lower misclassfication rate then let me tell you \n",
    "above quoted things are for training dataset. Lower misclassification \n",
    "on training dataset doesn’t mean lower misclassification on validation/testing data.\n",
    "To get a better a result of testing data, SVM looks for a higher margin.\n",
    "\n",
    "So I finally confused you in how to set up this 2 contradictory things now? \n",
    "The answer is parameter C.\n",
    "\n",
    "- Paramerter C:\n",
    "\n",
    "- Large Value of parameter C => small margin\n",
    "\n",
    "- Small Value of paramerter C => Large margin\n",
    "\n",
    "\n",
    "### How should you choose the value of C?\n",
    "\n",
    "There is no rule of thumb to choose a C value, it totally depends \n",
    "on your testing data. The only option I see is trying bunch of \n",
    "different values and choose the value which gives you lowest \n",
    "misclassification rate on testing data. I would suggest you to use\n",
    "gridsearchCV, in which you can directly give a list of different \n",
    "values parameter and it will tell you which value is best.\n",
    "\n",
    "### C controls the cost of misclassification on the training data.\n",
    "\n",
    "The goal of SVM is to find a hyperplane that would leave the widest\n",
    "possible \"cushion\" between input points from two classes. \n",
    "There is a tradeoff between \"narrow cushion, little / no mistakes\" \n",
    "and \"wide cushion, quite a few mistakes\".\n",
    "\n",
    "Learning algorithms are about generalizing from input data, not \n",
    "explaining it. This is not to mention that, \"thanks to\" the curse of \n",
    "dimensionality, in large number of dimensions training data can often\n",
    "be explained quite well by over fitting the model.\n",
    "\n",
    "Therefore, often times it is desirable to specifically allow some \n",
    "training points to be misclassified in order to have an\n",
    "\"overall better\" position of the separating hyperplane.\n",
    "\n",
    "    Mathematically, \"better\" translates to \"optimizing cost function valuing mistakes with certain coefficient\".\n",
    "    Intuitively, \"better\" implies \"wider cushion, a few mistakes allowed\".\n",
    "    Practically \"better\" is to be understood as \"performs well on real data\". \n",
    "\n",
    "\n",
    "Small C makes the cost of misclassificaiton low (\"soft margin\"), \n",
    "thus allowing more of them for the sake of wider \"cushion\".\n",
    "\n",
    "Large C makes the cost of misclassification high ('hard margin\"),\n",
    "thus forcing the algorithm to explain the input data stricter and potentially overfit.\n",
    "\n",
    "The goal is to find the balance between \"not too strict\" and \"not too loose\". \n",
    "Cross-validation and resampling, along with grid search, \n",
    "are good ways to finding the best C.\n",
    "\n",
    "C is a parameter of the SVC learner and is the penalty for misclassifying a data point.\n",
    "                                                  \n",
    "When C is small, the classi er is okay with misclassi ed data points (high bias, low\n",
    "variance). When C is large, the classi er is heavily penalized for misclassi ed data and\n",
    "therefore bends over backwards avoid any misclassified data points (low bias, high\n",
    "variance).\n",
    "\n",
    "# gamma\n",
    "\n",
    "## Gamma\n",
    "-In the four charts below, we apply the same SVC-RBF classi er to the same data while\n",
    "holding C constant. The only difference between each chart is that each time we will\n",
    "increase the value of gamma . By doing so, we can visually see the effect of gamma on the\n",
    "decision boundary.\n",
    "                                                  \n",
    "## Gamma = 0.01\n",
    "-In the case of our SVC classi er and data, when using a low gamma like 0.01, the decision\n",
    "boundary is not very ‘curvy’, rather it is just one big sweeping arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()# we take this default data set\n",
    "\n",
    "#we want to prepare dataset\n",
    "\n",
    "#[:,:2] take two colums of the iris.data[] array  (data': array([[5.1, 3.5, 1.4, 0.2],) array give columsname to \n",
    "# array featurename [:2] default name \n",
    "\n",
    "   #below show the array in iris data\n",
    "    \n",
    " #'feature_names': ['sepal length (cm)',\n",
    "  #'sepal width (cm)',\n",
    " # 'petal length (cm)',\n",
    "  #'petal width (cm)'],\n",
    "    \n",
    "x=pd.DataFrame(iris.data[:,:2],columns=iris.feature_names[:2])\n",
    "#we take y as a target its always contain default array target in iris\n",
    "\n",
    "#'target': array([0, 0,]\n",
    "\n",
    "#so we make a data frame of target and give name species\n",
    "\n",
    "y=pd.DataFrame(iris.target,columns=[\"species\"],dtype='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR MACHINES(SVM): \n",
    "\n",
    "A support vector is used to compute the margin between \n",
    "two sides of data and maximize the margins. It draws a \n",
    "line to maintain the same exact distance from both of \n",
    "the sides. Which is also known for the best margin.\n",
    "\n",
    "- Every point in the margin is called a Vector.\n",
    "\n",
    "### Example: \n",
    "- Classification of Male & female diagram, we use SVM for better understanding.\n",
    "\n",
    "- After applying the kernel tricks method, we can do SVM and logistic regression.\n",
    "\n",
    "There are different methods & algorithms for every dataset. \n",
    "“One size does not fit for all” ->It means there is no single algorithm \n",
    "that will be for all universe. After all, we have to use different \n",
    "algorithms according to our requirements. It’s all a matter of choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel is used here as radio baise function rbf\n",
    "svc=svm.SVC(kernel=\"rbf\",C=2,gamma=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel is used here as linear gamma = 0.01\n",
    "svc=svm.SVC(kernel=\"linear\",C=1,gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=svm.SVC(kernel=\"rbf\",C=10,gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cryzal/ml/myenv/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.01, kernel='linear')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a mesh to plot in \n",
    "x_min,x_max = x.iloc[:,0].min() - 1, x.iloc[:,0].max() + 1\n",
    "y_min,y_max = x.iloc[:,1].min() - 1, x.iloc[:,1].max() + 1\n",
    "h = (x_max/x_min)/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx,yy=np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))\n",
    "z = svc.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a958b9e84fad4fe89d223f56a7adda42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-591d9e34d8ee>:3: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  ax.pcolormesh(xx,yy,z,alpha=0.8)\n"
     ]
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.pcolormesh(xx,yy,z,alpha=0.8)\n",
    "ax.scatter(x.iloc[:,0], x.iloc[:,1],c=y['species'].cat.codes,cmap=plt.cm.Paired)\n",
    "ax.set_xlabel(iris.feature_names[0])\n",
    "ax.set_ylabel(iris.feature_names[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
